{
 "cells": [
  {
   "source": [
    "### Import packages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pymysql\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "from pyspark.ml.feature import StringIndexer, Tokenizer, CountVectorizer, VectorAssembler, MinMaxScaler, IndexToString\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data from SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data_from_SQL(host, user, password, port, database):\n",
    "    dbcon = pymysql.connect(host=host ,user=user,password=password, port = port, database = database)\n",
    "    try:\n",
    "        pdf = pd.read_sql_query(\n",
    "            '''SELECT\n",
    "                name,\n",
    "                country,\n",
    "                industry\n",
    "              FROM company_table\n",
    "              ''', dbcon)\n",
    "        dbcon.close()\n",
    "        return(pdf)\n",
    "    except:\n",
    "        dbcon.close()\n",
    "        return(print(\"Error: unable to convert the data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "host='127.0.0.1'\n",
    "user='root'\n",
    "password=''\n",
    "port = 3307\n",
    "database = \"company_names\"\n",
    "\n",
    "pdf = import_data_from_SQL(host, user, password, port, database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distr(df):\n",
    "    indust_count = df['industry'].value_counts()\n",
    "    indust_count.plot.barh(x= indust_count.index, y=indust_count.values)\n",
    "    plt.gcf().set_size_inches(5, 60)\n",
    "    plt.grid()\n",
    "    return(plt.show())\n",
    "\n",
    "plot_class_distr(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Drop rows with 'nan', replace non eng char and remove space and beginnning and end \"\"\"\n",
    "\"\"\" Select the n largest classes and the rows with the classes. Undersample so that the largest classes have equal number of\n",
    "of instances as the smallest class -> Uniform data with same number of instances in each class.\"\"\"\n",
    "\n",
    "def clean_main_df(df, n):\n",
    "    df_copy = df.copy()\n",
    "    df_copy  = df_copy [(df_copy !='nan').all(1)]\n",
    "    df_copy['name'] = df_copy ['name'].str.replace('[^a-zA-Z]+',' ', regex=True).str.strip()\n",
    "    df_copy  = df_copy[df_copy['name'].str.isspace() == False]\n",
    "    df_copy  = df_copy .drop_duplicates()\n",
    "    indust_count = df_copy['industry'].value_counts()\n",
    "    n_indu = indust_count.index[:n]\n",
    "    df_sel_class = df_copy[df_copy['industry'].isin(n_indu)]\n",
    "    n_cut = df_sel_class['industry'].value_counts() - indust_count.loc[n_indu[-1]]\n",
    "    n_cut_dict = n_cut.to_dict()\n",
    "    for k, v in n_cut_dict.items():\n",
    "        df_class = df_sel_class[df_sel_class['industry'] == k]\n",
    "        drop_indices = np.random.choice(df_class.index, v, replace=False)\n",
    "        df_sel_class = df_sel_class.drop(drop_indices)\n",
    "    return(df_sel_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_classes = 3\n",
    "pdf_clean = clean_main_df(pdf, number_classes)\n",
    "sdf = spark.createDataFrame(pdf_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(sdf):\n",
    "    train, test = sdf.randomSplit([0.8, 0.2], seed=12345)\n",
    "\n",
    "    \"\"\" set country and industry to categorical columns \"\"\"\n",
    "    country_index_fit = StringIndexer(inputCol=\"country\", outputCol=\"country_index\").fit(sdf)\n",
    "    sdf_tr1 = country_index_fit.transform(train)\n",
    "    sdf_te1 = country_index_fit.transform(test)\n",
    "    industry_index_fit = StringIndexer(inputCol=\"industry\", outputCol=\"label\").fit(sdf_tr1)\n",
    "    sdf_tr2 = industry_index_fit.transform(sdf_tr1)\n",
    "    sdf_te2 = industry_index_fit.transform(sdf_te1)\n",
    "\n",
    "    \"\"\" convert name to countvector \"\"\"\n",
    "    tokenizer = Tokenizer(inputCol=\"name\", outputCol=\"token_name\")\n",
    "    sdf_tr3 = tokenizer.transform(sdf_tr2)\n",
    "    sdf_te3 = tokenizer.transform(sdf_te2)\n",
    "    cvector = CountVectorizer(inputCol=\"token_name\", outputCol=\"name_vector\", vocabSize=30).fit(sdf_tr3)\n",
    "    sdf_tr4 = cvector.transform(sdf_tr3)\n",
    "    sdf_te4 = cvector.transform(sdf_te3)\n",
    "\n",
    "    \"\"\" combine the features \"\"\"\n",
    "    assembler = VectorAssembler(inputCols=[\"country_index\", \"name_vector\"], outputCol=\"features\")\n",
    "    sdf_tr5 = assembler.transform(sdf_tr4)\n",
    "    sdf_te5 = assembler.transform(sdf_te4)\n",
    "\n",
    "    \"\"\" norm features \"\"\"\n",
    "    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "    scalerModel = scaler.fit(sdf_tr5)\n",
    "    sdf_tr6 = scalerModel.transform(sdf_tr5)\n",
    "    sdf_te6 = scalerModel.transform(sdf_te5)\n",
    "\n",
    "    columns_to_drop = [\"country_index\",\"token_name\",\"name_vector\", \"features\"]\n",
    "    sdf_tr7 = sdf_tr6.drop(*columns_to_drop)\n",
    "    sdf_te7 = sdf_te6.drop(*columns_to_drop)\n",
    "\n",
    "    return(sdf_tr7, sdf_te7, country_index_fit, industry_index_fit, tokenizer, cvector, assembler, scalerModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test, country_index_fit, industry_index_fit, tokenizer, cvector, assembler, scalerModel = transform_data(sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(training, test):\n",
    "    rf = RandomForestClassifier(featuresCol = 'scaledFeatures', labelCol = 'label')\n",
    "    rfModel = rf.fit(training)\n",
    "    predictions = rfModel.transform(test)\n",
    "    evaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "    predictionAndTarget = predictions.select(\"label\", \"prediction\")\n",
    "    acc = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"accuracy\"})\n",
    "    f1 = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"f1\"})\n",
    "    weightedPrecision = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"weightedPrecision\"})\n",
    "    weightedRecall = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"weightedRecall\"})\n",
    "    print('Accuracy = {} \\nF1 = {} \\nWeighted Precision = {} \\nWeighted Recall = {}'.format(round(acc,2), round(f1,2), round(weightedPrecision,2), round(weightedRecall,2)))\n",
    "    return(rfModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy = 0.54 \nF1 = 0.51 \nWeighted Precision = 0.64 \nWeighted Recall = 0.54\n"
     ]
    }
   ],
   "source": [
    "rfModel = create_model(training, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_input(rfModel, country_index_fit, industry_index_fit, tokenizer, cvector, assembler, scalerModel):\n",
    "    name = input('Name: ').lower()\n",
    "    country = input('Country: ').lower()\n",
    "    input_sdf = spark.createDataFrame([(name, country)],['name', 'country'])\n",
    "    input_sdf1 = country_index_fit.transform(input_sdf)\n",
    "    input_sdf2 = tokenizer.transform(input_sdf1)\n",
    "    input_sdf3 = cvector.transform(input_sdf2)\n",
    "    input_sdf4 = assembler.transform(input_sdf3)\n",
    "    input_sdf5 = scalerModel.transform(input_sdf4)\n",
    "    columns_to_drop = [\"country_index\",\"token_name\",\"name_vector\", \"features\"]\n",
    "    input_sdf6 = input_sdf5.drop(*columns_to_drop)\n",
    "    input_predictions = rfModel.transform(input_sdf6)\n",
    "    inp_pred = input_predictions.select('prediction')\n",
    "    inp_prob = input_predictions.select('probability')\n",
    "    inverter = IndexToString(inputCol=\"prediction\", outputCol=\"industry_pred\", labels=industry_index_fit.labels)\n",
    "    itd = inverter.transform(input_predictions)\n",
    "    input_pred = itd.select('probability').collect()[0][0]\n",
    "    indust_lab = industry_index_fit.labels\n",
    "    for i,j in zip(input_pred,indust_lab):\n",
    "        print('{} {} %'.format(j,round(i,2)*100))\n",
    "    return(print(itd.select('name', 'country', 'industry_pred').show()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "construction 26.0 %\n",
      "information technology and services 40.0 %\n",
      "marketing and advertising 34.0 %\n",
      "+----+-------+--------------------+\n",
      "|name|country|       industry_pred|\n",
      "+----+-------+--------------------+\n",
      "|wuzu|denmark|information techn...|\n",
      "+----+-------+--------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "check_input(rfModel, country_index_fit, industry_index_fit, tokenizer, cvector, assembler, scalerModel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('.venv')",
   "metadata": {
    "interpreter": {
     "hash": "080c9ccaea4eda24bbf18222e1441900524be6b0ef54035ee4e83413e32f5c67"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}